---
title: "Neural mass model simulation and external stimulation parameter recovery with Stan"
---

```{r}
library(tidygraph)
library(ggraph)
library(gridExtra)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
source(paste0(helpers_path,'run_ucr_glm.R'))
source(paste0(helpers_path,'run_ext_glm.R'))
source(paste0(helpers_path,'check_net_resids.R'))
```

# Developing intuitions for activity propogation within a network

## Non-hub stimulation

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
edges %>%
  rbind(data.frame(from = c(1,2,3,3,1,3), to = c(1,2,3,1,3,2), weight = c(0,0,0,0,0,0))) %>%
  ggplot(aes(x=from, y=to, fill=weight))+
  geom_tile()
```

Suppose we have a task that looks like:

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))
task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

The task stimulates *only* node 1 directly and propagates within the network based on the following Eq:

$$\frac{dx_i}{dt}\tau_i = -x_i(t) + s\phi\big(x_i(t)\big) + g\Bigg(\sum_{j\neq i}^{N} W_{ij}\phi\big(x_j(t)\big)\Bigg) + I_i(t)$$
Activity in each node at each time point would look like:

```{r}
cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$stim_node = 1
cur_args_dict$I = make_stimtimes(cur_args_dict$stim_node, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Hub stimulation

The stim node in the previous stimulations was **not** the hub. What if you stimulate the hub node? (In this network this would be node 2)

```{r}
hub_args_dict = cur_args_dict

hub_args_dict$stim_node = 2
hub_args_dict$I = make_stimtimes(hub_args_dict$stim_node, hub_args_dict)$stimtimes

hub_net_dat = networkModel(W, hub_args_dict)

data.frame(hub_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Longer stimulation

What if the task is on for longer?

```{r}
longon_task = data.frame(stim = c(c(0,0,0), rep(1, 10), rep(0,100)))
longon_task$time = rep(1:nrow(longon_task))
longon_task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

```{r}
longon_args_dict = cur_args_dict
longon_args_dict$tasktiming = longon_task$stim
longon_args_dict$Tmax = max(longon_task$time)
longon_args_dict$stim_node = 2
longon_args_dict$I = make_stimtimes(longon_args_dict$stim_node, longon_args_dict)$stimtimes

longon_net_dat = networkModel(W, longon_args_dict)

data.frame(longon_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop stimulation

What if you have complete loop?

```{r}
loop_W = W
loop_W[2, 3] = .2
loop_W
```

Such that the adjacency matrix changes to:

```{r}
data.frame(loop_W) %>%
  mutate(to = row.names(.),
         from = names(.),
         from = gsub("X","",from),
         to = as.numeric(to),
         from = as.numeric(to)) %>%
  gather(key, weight, -to, -from) %>%
  mutate(from = sort(from)) %>%
  select(-key) %>%
  mutate(border = ifelse(to == 2 & from == 3, T, NA)) %>%
  ggplot(aes(x=from, y=to, fill=weight, color=border))+
  geom_tile(size = 2) +
  scale_color_manual(guide=F, values = c('TRUE' = "red"))
```

Then activity does not turn off down to 0 but settles at an intermediate value.

```{r}
loop_args_dict = longon_args_dict
loop_args_dict$W = loop_W

loop_net_dat = networkModel(loop_W, loop_args_dict)

data.frame(loop_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop short task stimulation

Looks weird but keeping it for later tests.

```{r}
loop_short_args_dict = loop_args_dict
loop_short_args_dict$tasktiming = task$stim
loop_short_args_dict$Tmax = max(task$time)
loop_short_args_dict$I = make_stimtimes(loop_short_args_dict$stim_node, loop_short_args_dict)$stimtimes

loop_short_net_dat = networkModel(loop_W, loop_short_args_dict)

data.frame(loop_short_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

# Task parameter recovery

## GLM framework

Ok so now that you have a sense of how things work in the network try the regression framework in the initial simple network to see if you can detect which node is stimulated versus which is not.

Based on the algebra [here](https://github.com/zenkavi/NetworkGLM/blob/master/simulations/NB0_Inverting_Runge_Kutte.ipynb).

$$x_{i}(t+dt) = (1-\frac{dt}{\tau_i}+\frac{dt^2}{2\tau_i^2}){x_{i}(t)} + \frac{dt}{2\tau_i}\Bigg[(1 - \frac{dt}{\tau_i})\Bigg(gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg) + gN_i(t+dt) + s\phi\Bigg((1-\frac{dt}{\tau_i})x_{i}(t) + \frac{dt}{\tau_i}\big[gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\big]\Bigg) + {I}_{i}(t+dt)\Bigg]$$

### Non-hub stimulation

```{r}
net_dat_debug = networkModel(W, cur_args_dict, debug=T)
```

"Uncorrected" GLM overestimates task parameter. Task betas for each node:

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
round(run_ucr_glm(net_dat_debug, task_reg = cur_args_dict$I[1,])$ucr_task_betas, 4)
```

"Extended" GLM recovers correct task parameter.

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$ext_task_betas, 4)
```

### Hub stimulation

```{r}
hub_net_dat_debug = networkModel(W, hub_args_dict, debug=T)
```

Extended GLM correctly recovers task parameters when the hub node is stimulated.

```{r}
print(paste0("Stim node is: ", hub_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(hub_net_dat_debug, task_reg = hub_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(hub_net_dat_debug, cur_args_dict, task_reg = hub_args_dict$I[2,])$ext_task_betas, 4)
```

### Longer stimulation

```{r}
longon_net_dat_debug = networkModel(W, longon_args_dict, debug=T)
```

**Extended GLM overcorrects non-stimulated nodes.**

```{r}
print(paste0("Stim node is: ", longon_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(longon_net_dat_debug, task_reg = longon_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_task_betas, 4)
```

What's going on with the overcorrected nodes? Why does the model think the estimate of x_t_dt is better when you multiply I_t_dt with -0.0001 instead of 0? Because when estimating the model for non-stimulated nodes we use the tasktiming as the task regressor even though the true value for this is 0. We make this choice because apriori we can't know or measure which nodes would be stim nodes. This choice affects three regressors: `I_t`, `I_t_dt` and `s_phi_ave`. The model tries to balance the false increase in all three of these regressors. The difference between the shapes of `I_t` and `I_t_dt` used in the network activity creation and the corrected regression are trivial (for non-stim nodes it should be 0 but the task regressor fed into the regression model is told that the task is on) and therefore not depicted below. The effect on `s_phi_ave` is harder to visualize so it is shown below. For a non-trivial number of time points the regression `s_phi_ave` is larger than the `s_phi_ave` used in the network activity creation. To balance this overestimation the model assigns negative coefficients to the other two task regressors which appears like an "overcorrection"

```{r  fig.asp=.3, fig.width=6}
tmp_df = run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, longon_net_dat_debug, longon_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1)) +
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

Why don't we observe the same problem when the stimulation is short? It might be because the short task doesn't lead to a systematic/large enough deviation between the network `s_phi_ave` and regression `s_phi_ave`

```{r fig.asp=.3, fig.width=6}
tmp_df = run_ext_glm(hub_net_dat_debug, hub_args_dict, task_reg = hub_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, hub_net_dat_debug, hub_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))+
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

### Loop stimulation

```{r}
loop_net_dat_debug = networkModel(loop_W, loop_args_dict, debug=T)
```

Extended GLM overcorrects non-stimulated nodes. But this is due to the longer task and not due to the loop. See below for the same loop adjacency matrix with a shorter task. The corrected estimates are not over-corrected for non-stimulated nodes.

```{r}
print(paste0("Stim node is: ", loop_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(loop_net_dat_debug, task_reg = loop_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(loop_net_dat_debug, loop_args_dict, task_reg = loop_args_dict$I[2,])$ext_task_betas, 4)
```

### Loop short stimulation

```{r}
loop_short_net_dat_debug = networkModel(loop_W, loop_short_args_dict, debug=T)
```

```{r}
print(paste0("Stim node is: ", loop_short_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(loop_short_net_dat_debug, task_reg = loop_short_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(loop_short_net_dat_debug, loop_short_args_dict, task_reg = loop_short_args_dict$I[2,])$ext_task_betas, 4)
```

**What does the adjacency matrix look like if you estimate it from timeseries?**

True adjacency matrix

```{r}
cur_args_dict$W
```

Correlation matrix from observed time series

```{r}
round(cor(t(net_dat)), 4)
```

```{r}
round(ppcor::pcor(t(net_dat), method="kendall")$estimate, 4)
```

**How are the estimates of ext_glm affected if you use the estimated adjacency matrix**

```{r}

```

## Bayesian parameter recovery with Stan

Move on to estimating task parameters using Stan. 

```{r}
library(cmdstanr)
```

```{r}
mod = cmdstan_model("../../stanModels/neuralMass/nmm_ode.stan")
```

```{r}
nmm_data = list(N_TS = dim(net_dat)[2],
                N = dim(net_dat)[1],
                ts = 1:N_TS,
                y_init = net_dat[,1],
                y = net_dat[,1],
                W = cur_args_dict$W,
                I = cur_args_dict$I[1,])
```

Note: currently

```{r}
fit <- mod$sample(
  data = nmm_data,
  seed = 123
)
```

```{r}
fit$summary()
```
------

Question re estimation methods: Are the posteriors unimodal in VI vs MCMC? Are we losing important information when making assumptions like the Laplace approximation?
