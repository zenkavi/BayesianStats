---
title: "Neural mass model simulation and external stimulation parameter recovery with Stan"
---

```{r}
library(tidygraph)
library(ggraph)
library(gridExtra)
library(cmdstanr)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'plot_adj_mat.R'))
```

# Developing intuitions for activity propogation within a network

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
plot_adj_mat(W)$p
```

Suppose we have resting state data (i.e. no task).

Activity in each node at each time point would look like:

```{r}
cur_args_dict = list('dt'=.5,  
                 'noise'= TRUE,
                 'noise_loc'= 0, 
                 'noise_scale'= 0.1,
                 's'=.3,
                 'g'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tau'=1, 
                 'Tmax'=1000,
                 'W'= W)

# cur_args_dict$stim_node = 1
# cur_args_dict$I = make_stimtimes(cur_args_dict$stim_node, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

# Bayesian parameter recovery

```{r}
cat(paste0("True s is: ", cur_args_dict$s))
cat('\n')
cat(paste0("True g is: ", cur_args_dict$g))
cat('\n')
cat(paste0("True tau is: ", cur_args_dict$tau))
```

```{r}
nmm_data = list(N_TS = dim(net_dat)[2],
                N = dim(net_dat)[1],
                ts = 1:dim(net_dat)[2],
                y_init = net_dat[,1],
                y = t(net_dat),
                W = cur_args_dict$W)
```

```{r}
mod = cmdstan_model("stanModels/nmm_ode.stan")
```

```{r}
fit = mod$variational(data=nmm_data)
```

```{r}
fit
```
Overlay priors on this graph of posteriors

s ~ beta(1, 1);
g ~ beta(1, 1);
tau ~ normal(1, 0.5);
sigma ~ lognormal(-1, 1);

```{r}
as_draws_df(fit$draws()) %>%
  select(s, g, tau, sigma) %>%
  gather(key, value) %>%
  mutate(type="posterior") %>%
  ggplot()+
  geom_density(aes(value))+
  facet_wrap(~key, scales='free')
```

```{r}
min_max_df = as_draws_df(fit$draws()) %>%
  select(s, g, tau, sigma) %>%
  gather(key, value) %>%
  group_by(key) %>%
  summarise(min_val = min(value),
            max_val = max(value))

prior_df = data.frame(par=rep(NA, 4000), vals = rep(NA, 4000), dens = rep(NA, 4000))

for(i in 1:4){

  prior_df$par[1+(1000*(i-1)):i*1000] = min_max_df$key[i]
  prior_df$par[1+(1000*(i-1)):i*1000]
  
  if(min_max_df$key[i]=='s'){
    prior_df$s = seq(min_max_df$min_val[i], min_max_df$max_val[i], length=1000)
    prior_df$d_s = dunif(prior_df$s)
  }
  if(min_max_df$key[i]=='g'){
    prior_df$g = seq(min_max_df$min_val[i], min_max_df$max_val[i], length=1000)
    prior_df$d_g = dunif(prior_df$g)
  }
  if(min_max_df$key[i]=='sigma'){
    prior_df$sigma = seq(min_max_df$min_val[i], min_max_df$max_val[i], length=1000)
    prior_df$d_sigma = dlnorm(prior_df$sigma, -1, 1)
  }
  if(min_max_df$key[i]=='tau'){
    prior_df$tau = seq(min_max_df$min_val[i], min_max_df$max_val[i], length=1000)
    prior_df$d_tau = dnorm(prior_df$tau, 1, .5)
  }
}

prior_df %>%
  gather(key, value, -type) %>%
   ggplot()+
  facet_wrap(~key, scales='free')
```

MCMC is still insanely slow. 

```{r}
fit_mcmc <- mod$sample(
  data = nmm_data,
  seed = 123
)
```

```{r}
fit_mcmc
```

```{r}
fit
```


------

Question re estimation methods: Are the posteriors unimodal in VI vs MCMC? Are we losing important information when making assumptions like the Laplace approximation?
