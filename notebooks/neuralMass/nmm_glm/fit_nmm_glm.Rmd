---
title: "Neural mass model simulation and external stimulation parameter recovery with GLM"
---

```{r message=FALSE, warning=FALSE}
library(tidygraph)
library(ggraph)
library(gridExtra)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
source(paste0(helpers_path,'run_ucr_glm.R'))
source(paste0(helpers_path,'run_ext_glm.R'))
source(paste0(helpers_path,'check_net_resids.R'))
source(paste0(helpers_path,'plot_adj_mat.R'))
```

# Intuitions for activity propogation within a network

## Non-hub stimulation

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
out = plot_adj_mat(W)
print(out$p)
```

Suppose we have a task that looks like:

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))
task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

The task stimulates *only* node 1 directly and propagates within the network based on the following Eq:

$$\frac{dx_i}{dt}\tau_i = -x_i(t) + s\phi\big(x_i(t)\big) + g\Bigg(\sum_{j\neq i}^{N} W_{ij}\phi\big(x_j(t)\big)\Bigg) + I_i(t)$$
where the self-stimulation `s` and network-stimulation `g` parameters are:

```{r}
cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$stim_node = 1
cur_args_dict$I = make_stimtimes(cur_args_dict$stim_node, cur_args_dict)$stimtimes

cat(paste0('Self-stimulation parameter s is: ', cur_args_dict$s))
cat("\n")
cat(paste0('Network stimulation parameter g is: ', cur_args_dict$g))
```

Activity in each node at each time point would look like:

```{r}
net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Hub stimulation

The stim node in the previous stimulations was **not** the hub. What if you stimulate the hub node? (In this network this would be node 2)

```{r}
hub_args_dict = cur_args_dict

hub_args_dict$stim_node = 2
hub_args_dict$I = make_stimtimes(hub_args_dict$stim_node, hub_args_dict)$stimtimes

hub_net_dat = networkModel(W, hub_args_dict)

data.frame(hub_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Longer stimulation

What if the task is on for longer?

```{r}
longon_task = data.frame(stim = c(c(0,0,0), rep(1, 10), rep(0,100)))
longon_task$time = rep(1:nrow(longon_task))
longon_task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

```{r}
longon_args_dict = cur_args_dict
longon_args_dict$tasktiming = longon_task$stim
longon_args_dict$Tmax = max(longon_task$time)
longon_args_dict$stim_node = 2
longon_args_dict$I = make_stimtimes(longon_args_dict$stim_node, longon_args_dict)$stimtimes

longon_net_dat = networkModel(W, longon_args_dict)

data.frame(longon_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop stimulation

What if you have complete loop?

```{r}
loop_W = W
loop_W[2, 3] = .2
loop_W
```

Such that the adjacency matrix changes to:

```{r}
out = plot_adj_mat(loop_W, border_to=2, border_from=3)
out$p
```

Then activity does not turn off down to 0 but settles at an intermediate value.

```{r}
loop_args_dict = longon_args_dict
loop_args_dict$W = loop_W

loop_net_dat = networkModel(loop_W, loop_args_dict)

data.frame(loop_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop short task stimulation

Looks weird but keeping it for later tests.

```{r}
loop_short_args_dict = loop_args_dict
loop_short_args_dict$tasktiming = task$stim
loop_short_args_dict$Tmax = max(task$time)
loop_short_args_dict$I = make_stimtimes(loop_short_args_dict$stim_node, loop_short_args_dict)$stimtimes

loop_short_net_dat = networkModel(loop_W, loop_short_args_dict)

data.frame(loop_short_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

# Task parameter recovery

## GLM framework

Ok so now that you have a sense of how things work in the network try the regression framework in the initial simple network to see if you can detect which node is stimulated versus which is not.

Based on the algebra [here](https://github.com/zenkavi/NetworkGLM/blob/master/simulations/NB0_Inverting_Runge_Kutte.ipynb).

$$x_{i}(t+dt) = (1-\frac{dt}{\tau_i}+\frac{dt^2}{2\tau_i^2}){x_{i}(t)} + \frac{dt}{2\tau_i}\Bigg[(1 - \frac{dt}{\tau_i})\Bigg(gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg) + gN_i(t+dt) + s\phi\Bigg((1-\frac{dt}{\tau_i})x_{i}(t) + \frac{dt}{\tau_i}\big[gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\big]\Bigg) + {I}_{i}(t+dt)\Bigg]$$

Given the parameters set in the stimulation

```{r}
cat(paste0('Sampling rate dt is: ', cur_args_dict$dt))
cat("\n")
cat(paste0('Time constant tau is: ', cur_args_dict$tau))
```

and the above equation used for the GLM inversion the true task effect for stimulated nodes is the coefficient of the ${I}_{i}(t+dt)$ regressor which is $\frac{dt}{2\tau_i}$

```{r}
cur_args_dict$dt / (2*cur_args_dict$tau)
```

and 0 for non-stimulated nodes.

### Non-hub stimulation

```{r}
net_dat_debug = networkModel(W, cur_args_dict, debug=T)
```

"Uncorrected" GLM overestimates task parameter. Task betas for each node:

```{r}
cat(paste0("Stim node is: ", cur_args_dict$stim_node))
cat("\n")
cat("Uncorrected estimates for each node:")
cat("\n")
round(run_ucr_glm(net_dat_debug, task_reg = cur_args_dict$I[1,])$ucr_task_betas, 4)
```

"Extended" GLM recovers correct task parameter.

```{r}
cat(paste0("Stim node is: ", cur_args_dict$stim_node))
cat("\n")
cat("Corrected estimates for each node:")
cat("\n")
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$ext_task_betas, 4)
```

### Hub stimulation

```{r}
hub_net_dat_debug = networkModel(W, hub_args_dict, debug=T)
```

Extended GLM correctly recovers task parameters when the hub node is stimulated.

```{r}
cat(paste0("Stim node is: ", hub_args_dict$stim_node))
cat("\n")
cat("Uncorrected estimates:")
cat("\n")
round(run_ucr_glm(hub_net_dat_debug, task_reg = hub_args_dict$I[2,])$ucr_task_betas, 4)
cat("\n")
cat("Corrected estimates:")
cat("\n")
round(run_ext_glm(hub_net_dat_debug, cur_args_dict, task_reg = hub_args_dict$I[2,])$ext_task_betas, 4)
```

### Longer stimulation

```{r}
longon_net_dat_debug = networkModel(W, longon_args_dict, debug=T)
```

**Extended GLM overcorrects non-stimulated nodes.**

```{r}
cat(paste0("Stim node is: ", longon_args_dict$stim_node))
cat("\n")
cat("Uncorrected estimates:")
cat("\n")
round(run_ucr_glm(longon_net_dat_debug, task_reg = longon_args_dict$I[2,])$ucr_task_betas, 4)
cat("\n")
cat("Corrected estimates:")
cat("\n")
round(run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_task_betas, 4)
```

What's going on with the overcorrected nodes? Why does the model think the estimate of `x_t_dt` is better when you multiply `I_t_dt` with -0.0001 instead of 0? Because when estimating the model for non-stimulated nodes we use the tasktiming as the task regressor even though the true value for this is 0. We make this choice because apriori we can't know or measure which nodes would be stim nodes. This choice affects three regressors: `I_t`, `I_t_dt` and `s_phi_ave`. The model tries to balance the false increase in all three of these regressors. 

The difference between the shapes of `I_t` and `I_t_dt` used in the network activity creation and the corrected regression are trivial (for non-stim nodes it should be 0 but the task regressor fed into the regression model is told that the task is on) and therefore not depicted below.    

The effect on `s_phi_ave` is harder to visualize so it is shown below. For a non-trivial number of time points the regression `s_phi_ave` is larger than the `s_phi_ave` used in the network activity creation. To balance this overestimation the model assigns negative coefficients to the other two task regressors (`I_t`, `I_t_dt`) which appears like an "overcorrection" for the `I_t_dt` which is used as the task effect proxy.

```{r  fig.asp=.3, fig.height=10}
tmp_df = run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, longon_net_dat_debug, longon_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1)) +
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

Why don't we observe the same problem when the stimulation is short? Because the short task doesn't lead to a systematic/large enough deviation between the network `s_phi_ave` and regression `s_phi_ave`

```{r fig.asp=.3, fig.height=10}
tmp_df = run_ext_glm(hub_net_dat_debug, hub_args_dict, task_reg = hub_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, hub_net_dat_debug, hub_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))+
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

### Loop stimulation

```{r}
loop_net_dat_debug = networkModel(loop_W, loop_args_dict, debug=T)
```

Extended GLM overcorrects non-stimulated nodes. But this is due to the longer task and not due to the loop. See below for the same loop adjacency matrix with a shorter task. The corrected estimates are not over-corrected for non-stimulated nodes.

```{r}
cat(paste0("Stim node is: ", loop_args_dict$stim_node))
cat("\n")
cat("Uncorrected estimates:")
cat("\n")
round(run_ucr_glm(loop_net_dat_debug, task_reg = loop_args_dict$I[2,])$ucr_task_betas, 4)
cat("\n")
cat("Corrected estimates:")
cat("\n")
round(run_ext_glm(loop_net_dat_debug, loop_args_dict, task_reg = loop_args_dict$I[2,])$ext_task_betas, 4)
```

### Loop short stimulation

```{r}
loop_short_net_dat_debug = networkModel(loop_W, loop_short_args_dict, debug=T)
```

```{r}
cat(paste0("Stim node is: ", loop_short_args_dict$stim_node))
cat("\n")
cat("Uncorrected estimates:")
round(run_ucr_glm(loop_short_net_dat_debug, task_reg = loop_short_args_dict$I[2,])$ucr_task_betas, 4)
cat("\n")
cat("Corrected estimates:")
cat("\n")
round(run_ext_glm(loop_short_net_dat_debug, loop_short_args_dict, task_reg = loop_short_args_dict$I[2,])$ext_task_betas, 4)
```

## Larger network

Does the extended GLM framework work for a larger network with a topdown block task?

```{r}
big_W_args_dict = default_args_dict

#Change network such that most of the activity is driven by network compared to self stimulation
big_W_args_dict$s = .7
big_W_args_dict$g = 1
big_W_args_dict$innetwork_dsity = .85
big_W_args_dict$hubnetwork_dsity = .0
big_W_args_dict$outnetwork_dsity = .0005
  

big_G = generateStructuralNetwork(big_W_args_dict)
big_W_args_dict$W = generateSynapticNetwork(big_G$W, showplot=big_W_args_dict$showplot)$G
big_W_args_dict$stim_nodes = c(1:10)
big_W_args_dict$I = make_stimtimes(big_W_args_dict$stim_nodes, big_W_args_dict)$stimtimes
big_W_net_dat = networkModel(big_W_args_dict$W, big_W_args_dict)
big_W_net_dat_debug = networkModel(big_W_args_dict$W, big_W_args_dict, debug=T)
```

Suppose we have a network of 105 nodes with 3 communities of 35 nodes each. This network has one hub and two local communities. The hub community is more likely to be connected to the local community nodes, while local communities only have high connectivity within their own networks.

```{r}
plot_adj_mat(big_W_args_dict$W)$p
```

The block task comes on first after 100 time points and then after every 400 time points for 100 time points. It is top-down, meaning it only stimulates some (10) nodes in the hub network directly.

```{r}
data.frame(stim = make_stimtimes(big_W_args_dict$stim_nodes, big_W_args_dict)$tasktiming) %>%
  mutate(timepoint = 1:n())%>%
  ggplot(aes(timepoint, stim)) +
  geom_line()
```

Activity propagates in this network based on the following self and network stimulation parameters

```{r}
cat(paste0("Self stimulation parameter s is: ", big_W_args_dict$s))
cat("\n")
cat(paste0("Network stimulation parameter g is: ", big_W_args_dict$g))
```

First check network structure. Confirm that the number of total connections to stim nodes and the sum of weights from stim nodes is larger for non-stim hub nodes than they are for non stim local nodes.

```{r}
#from in columns; to in rows
dist_mat = igraph::distances(igraph::graph_from_adjacency_matrix(big_W_args_dict$W, weighted=T), algorithm = 'johnson')

tmp = data.frame(sum_stim_node_cons = rowSums(big_G$W[11:105, 1:10]), 
           sum_stim_node_weights = rowSums(big_W_args_dict$W[11:105, 1:10]),
           mean_distance = rowMeans(dist_mat[11:105 , 1:10])) %>%
  mutate(node = 1:n(),
         node = node+10,
         comm = ifelse(node<36, 1, ifelse(node>35 & node<71, 2, 3)))


tmp %>%
  gather(key, value, -node, -comm) %>%
  ggplot(aes(node, value))+
  geom_line()+
  geom_vline(aes(xintercept = 35), linetype="dashed")+
  geom_vline(aes(xintercept = 70), linetype="dashed")+
  facet_grid(key~., scales='free_y', switch = 'both')+
  scale_y_continuous(position="right")+
  ylab("")
```

```{r}
tmp %>%
  gather(key, value, -node, -comm) %>%
  ggplot(aes(as.factor(comm), value))+
  geom_boxplot(width=.3)+
  facet_grid(key~., scales='free_y', switch = 'both')+
  scale_y_continuous(position="right")+
  xlab("Community")+
  ylab("")+
  geom_hline(aes(yintercept=0))+
  theme(axis.line.x = element_blank(),
        axis.ticks.x = element_blank())
```

The structure of the network looks fine based on the above. So where is the bleeding supposed to be?  

What is bleeding? It is the higher overestimation of task activity to nodes closer (/directly connected) to stimulated nodes in UCR GLM. Since the task regressor used for all nodes in the same difference in relationship can only be due to difference on nodes' timeseries.

Is the time series of non-stim hub nodes are more similar to stim hub nodes than the non-stim local community nodes (which as shown above are not as strongly connected to stim nodes) other nodes are to stim nodes?

*Even with little to no connectivity to the hub community the time series of distant nodes is very similar to stim nodes*

```{r}
similarity_df = data.frame(cor(t(big_W_net_dat))[11:105,1:10]) %>%
  mutate(node = 1:n(),
         node = node+10) %>%
  gather(key, similarity, -node) %>%
  mutate(key = gsub("X", "node_", key))

similarity_df %>%
  ggplot(aes(node, similarity, color=key))+
  geom_line()+
  ylab("Similarity to stim node")+
  theme(legend.title=element_blank())+
  geom_vline(aes(xintercept = 35), linetype="dashed")+
  geom_vline(aes(xintercept = 70), linetype="dashed")

```
Does the similarity of a non-stim timeseries to stim timeseries depend on its distance from stim node?

```{r}
data.frame(dist_mat[11:105 , 1:10]) %>%
  mutate(node = 1:n(),
         node = node+10) %>%
  gather(key, distance, -node) %>%
  mutate(key = gsub("X", "node_", key)) %>%
  left_join(similarity_df, by=c("node", "key")) %>%
  mutate(community = as.factor(ifelse(node<36,1, ifelse(node>35&node<71,2,3)))) %>%
  ggplot(aes(distance, similarity, color=community))+
  geom_point()
  
```
```{r}
print(paste0("Stim node is: ", big_W_args_dict$stim_nodes))
cat("\n")
cat("Uncorrected estimates")
cat("\n")
round(run_ucr_glm(big_W_net_dat_debug, task_reg = big_W_args_dict$I[1,])$ucr_task_betas, 4)
cat("\n")
cat("Corrected estimates with network activity")
cat("\n")
round(run_ext_glm(big_W_net_dat_debug, big_W_args_dict, task_reg = big_W_args_dict$I[1,])$ext_task_betas, 4)
cat("\n")
cat("Corrected estimates without network activity")
cat("\n")
round(run_ext_glm(big_W_net_dat_debug, big_W_args_dict, task_reg = big_W_args_dict$I[1,], inc_net_act = FALSE)$ext_task_betas, 4)
```

Plotting as I did before to visually detect if there is any "bleeding" to non-stim hub nodes. 

```{r}
data.frame(ucr_betas = run_ucr_glm(big_W_net_dat_debug, task_reg = big_W_args_dict$I[1,])$ucr_task_betas,
           ext_betas = run_ext_glm(big_W_net_dat_debug, big_W_args_dict, task_reg = big_W_args_dict$I[1,])$ext_task_betas) %>%
  mutate(node = 1:n()) %>%
  gather(key, beta, -node) %>%
  ggplot(aes(node, beta, col=key))+
  geom_line()+
  theme(legend.title = element_blank())+
  geom_vline(aes(xintercept = 35), linetype="dashed")+
  geom_vline(aes(xintercept = 70), linetype="dashed")
```

**WHERE HAS THE BLEEDING IN UCR ESTIMATES GONE?**

Plot overestimation by distance from stim/hub node?

x = distance (average distance from stimulated nodes)
y = overestimation (ucr - ext)

```{r}

```