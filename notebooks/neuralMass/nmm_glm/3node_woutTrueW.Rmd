---
title: "Accounting for network connectivity without true adjacency matrix"
---

```{r}
library(tidygraph)
library(ggraph)
library(gridExtra)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
source(paste0(helpers_path,'run_ucr_glm.R'))
source(paste0(helpers_path,'run_ext_glm.R'))
source(paste0(helpers_path,'check_net_resids.R'))
source(paste0(helpers_path,'plot_adj_mat.R'))
source('~/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/g_legend.R')
```

# Potential applicability issues

## Observable vs true adjacency matrix

Starting again with a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
out = plot_adj_mat(W)
print(out$p)
```
With a task that has a single stimulated time point

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))

cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$stim_node = 1
cur_args_dict$I = make_stimtimes(cur_args_dict$stim_node, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)
net_dat_debug = networkModel(W, cur_args_dict, debug=T)

print(paste0('Self-stimulation parameter s is: ', cur_args_dict$s))
print(paste0('Network stimulation parameter g is: ', cur_args_dict$g))
```


What does the "functional connectivity" matrix look like if you estimate it from timeseries?

**Note**: I haven't even considered inhibitory connections yet.

True adjacency matrix

```{r}
true_am = plot_adj_mat(cur_args_dict$W)
true_am_plot = true_am$p+
  scale_fill_gradient(limits=c(-1,1)) +
  ggtitle("True")

mylegend<-g_legend(true_am_plot)

```

"Functional" correlation matrix from observed time series

```{r}
methods = c("pearson", "spearman", "kendall")
ps = list()
for(i in 1:length(methods)){
  ps[[i]] = true_am_plot +
    theme(legend.position = "none")+
    ggtitle("True")
  ps[[i+length(methods)]] = plot_adj_mat(cor(t(net_dat), method=methods[i]))$p +
    scale_fill_gradient(limits=c(-1,1))+
    ggtitle(paste0(methods[i], " (full)"))+
    theme(legend.position = "none")
  ps[[i+2*length(methods)]] = plot_adj_mat(ppcor::pcor(t(net_dat), method=methods[i])$estimate)$p +
    scale_fill_gradient(limits=c(-1,1)) +
    ggtitle(paste0(methods[i], " (partial)"))+
    theme(legend.position = "none")
}
```

All three plots in first row are identical and third row is partial correlations.

```{r out.height='100%'}
grid.arrange(arrangeGrob(grobs = ps, nrow=3), mylegend, ncol=2, widths = c(10,1))
```

How similar is the "functional connectivity" matrices to the true connectivity matrix? Not very.

```{r}
for(i in 1:length(methods)){
  cat(paste0("r between W and ", methods[i], " - full: "))
  cat(cor(c(W), c(cor(t(net_dat), method=methods[i]))))
  cat("\n")
  cat(paste0("r between W and ", methods[i], " - partial: "))
  cat(cor(c(W), c(ppcor::pcor(t(net_dat), method=methods[i])$estimate)))
  cat("\n")
}

```

## Effect on ext_glm estimates

How are the estimates of ext_glm affected if you use the estimated adjacency matrix?

```{r}
args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = cur_args_dict
  args_dicts[[i]]$W = cor(t(net_dat), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = cur_args_dict
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(net_dat), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(net_dat, args_dicts[[i]], task_reg = cur_args_dict$I[1,])$ext_task_betas, 4))
}

```

Does using different connectivity matrices affect other scenarios?

Hub stimulation

```{r}
sub1 = hub_args_dict
sub2 = hub_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Longer task with hub stimulation

```{r}
sub1 = longon_args_dict
sub2 = longon_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Looped network with longer task and hub stimulation

```{r}
sub1 = loop_args_dict
sub2 = loop_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Good news? Using different adjacency matrices doesn't seem to affect the results too much. How come?  

What regressors are affected by the change in the connectivity matrix? Given the model  

`mod = lm(x_t_dt ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + g_N_t_dt + s_phi_ave + I_t_dt)`  

affected regressors would be: `g_N_t`, `g_N_t_dt`, `s_phi_ave`  

So can you get significant improvement in estimating task involvement of each node just by accounting for self activity and not accounting for network activity? Yes.

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
print("Uncorrected estimates")
round(run_ucr_glm(net_dat_debug, task_reg = cur_args_dict$I[1,])$ucr_task_betas, 4)
print("Corrected estimates with network activity")
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$ext_task_betas, 4)
print("Corrected estimates without network activity")
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,], inc_net_act = FALSE)$ext_task_betas, 4)
```

Does the addition of the network activity regressors improve the model significantly? It does. But the parameter estimates for task involvement do not change drastically compared to not including them.

```{r}
out = run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])

for (i in 1:length(out$ext_mods)){
  cur_df = out$ext_mods[[i]]
  m1 = lm(x_t_dt ~ -1 + x_t + s_phi_x_t + I_t + I_t_dt, data = cur_df)
  m2 = lm(x_t_dt ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + g_N_t_dt + s_phi_ave + I_t_dt, data = cur_df)
  print(anova(m1, m2))
}
```

# `s` and `g` effects on extended GLM

## 3 node network

What is the effect of removing network activity from the extended regression depending `s` to `g` (ie if the network is driven more or less by the network compared to self stimulation)

```{r}
# sweep_vals = c(seq(0.001, .1 ,.01), seq(1, 5, .5))
sweep_vals = c(seq(0,1.5,.1), seq(1, 5, .5))

copy_dict = cur_args_dict
stim_node = 1
adj_mat = W

out = data.frame(matrix(NA, nrow=length(sweep_vals)^2, ncol=14))

row_count = 1 
for(cur_s in 1:length(sweep_vals)){
  for(cur_g in 1:length(sweep_vals)){
    
    cur_sg_dict = copy_dict
    cur_sg_dict$s = sweep_vals[cur_s]
    cur_sg_dict$g = sweep_vals[cur_g]
    cur_sg_netact = networkModel(adj_mat, cur_sg_dict, debug=T)
    cur_ucr = run_ucr_glm(cur_sg_netact, task_reg = cur_sg_dict$I[stim_node,])$ucr_task_betas
    cur_ext = run_ext_glm(cur_sg_netact, cur_sg_dict, task_reg = cur_sg_dict$I[stim_node,])$ext_task_betas
    cur_ext_nonet = run_ext_glm(cur_sg_netact, cur_sg_dict, task_reg = cur_sg_dict$I[stim_node,], inc_net_act=FALSE)$ext_task_betas
    cur_ext_noself = run_ext_glm(cur_sg_netact, cur_sg_dict, task_reg = cur_sg_dict$I[stim_node,], inc_self_stim=FALSE)$ext_task_betas
    cur_row = c(sweep_vals[cur_s], sweep_vals[cur_g], cur_ucr, cur_ext, cur_ext_nonet, cur_ext_noself)
    out[row_count,] = cur_row
    row_count = row_count+1
  }
}
names(out) = c("s", "g" , "ucr1", "ucr2", "ucr3", "ext1", "ext2", "ext3", "ext_nonet1", "ext_nonet2", "ext_nonet3", "ext_noself1", "ext_noself2", "ext_noself3")

```

Heatmaps of deviance from the correct task estimate for the extended GLM model ran without either the network activity regressors or self-stimulation regressors.

```{r}
out %>%
  select(s, g, ext_nonet1, ext_noself1, ext_nonet2, ext_noself2, ext_nonet3, ext_noself3) %>%
  gather(key, value, -s, -g) %>%
  separate(key, into = c("key", "node"), "(?<=[a-z])(?=[0-9])") %>%
  mutate(dev = ifelse(node == "1", value-.25, value), #
         s = as.factor(round(s, 2)),
         g = as.factor(round(g, 2)),
         key = factor(key,levels = c("ext_nonet", "ext_noself"),
                      labels = c("W/out network activity", "W/out self stimulation")),
         node = factor(node, levels=c(1,2,3),
                       labels = c("Non-hub stim", "Hub non-stim", "Non-hub non-stim"))) %>%
  ggplot(aes(s, g, fill=dev))+
  geom_tile(size=.6)+
  facet_grid(node~key)+
  theme(panel.grid=element_blank(),
        axis.text.x = element_text(angle=90))+
  scale_fill_gradient2(low = "red", mid="white", high = "blue", midpoint=0,  limits=c(-1,1),
                       name="Over/under \nestimation")
```

What proportion of activity is driven by task versus network?
task driver: self stim + task stim
Can you work this out from the eq above?

```{r}


```

What drives the bleeding of task effect to nonstim nodes that are connected to stim nodes? How does it change depending on the `s` to `g` ratio?
Bleeding = more overestimation of task coefficients for non-stim nodes connected to stim nodes (e.g. other nodes in a hub community for a top-down task) compared to non-stim nodes without direct connections to stim nodes (maybe it's better to keep this continuous by using a metric like distance to stim node instead of a binary classification)

```{r}


```
