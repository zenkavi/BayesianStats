---
title: "Neural mass model simulation and external stimulation parameter recovery with GLM"
---

```{r}
library(tidygraph)
library(ggraph)
library(gridExtra)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
source(paste0(helpers_path,'run_ucr_glm.R'))
source(paste0(helpers_path,'run_ext_glm.R'))
source(paste0(helpers_path,'check_net_resids.R'))
source(paste0(helpers_path,'plot_adj_mat.R'))
source('~/Dropbox/PoldrackLab/SRO_Retest_Analyses/code/helper_functions/g_legend.R')
```

# Developing intuitions for activity propogation within a network

## Non-hub stimulation

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
out = plot_adj_mat(W)
print(out$p)
```

Suppose we have a task that looks like:

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))
task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

The task stimulates *only* node 1 directly and propagates within the network based on the following Eq:

$$\frac{dx_i}{dt}\tau_i = -x_i(t) + s\phi\big(x_i(t)\big) + g\Bigg(\sum_{j\neq i}^{N} W_{ij}\phi\big(x_j(t)\big)\Bigg) + I_i(t)$$
Activity in each node at each time point would look like:

```{r}
cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$stim_node = 1
cur_args_dict$I = make_stimtimes(cur_args_dict$stim_node, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Hub stimulation

The stim node in the previous stimulations was **not** the hub. What if you stimulate the hub node? (In this network this would be node 2)

```{r}
hub_args_dict = cur_args_dict

hub_args_dict$stim_node = 2
hub_args_dict$I = make_stimtimes(hub_args_dict$stim_node, hub_args_dict)$stimtimes

hub_net_dat = networkModel(W, hub_args_dict)

data.frame(hub_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Longer stimulation

What if the task is on for longer?

```{r}
longon_task = data.frame(stim = c(c(0,0,0), rep(1, 10), rep(0,100)))
longon_task$time = rep(1:nrow(longon_task))
longon_task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

```{r}
longon_args_dict = cur_args_dict
longon_args_dict$tasktiming = longon_task$stim
longon_args_dict$Tmax = max(longon_task$time)
longon_args_dict$stim_node = 2
longon_args_dict$I = make_stimtimes(longon_args_dict$stim_node, longon_args_dict)$stimtimes

longon_net_dat = networkModel(W, longon_args_dict)

data.frame(longon_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop stimulation

What if you have complete loop?

```{r}
loop_W = W
loop_W[2, 3] = .2
loop_W
```

Such that the adjacency matrix changes to:

```{r}
out = plot_adj_mat(loop_W, border_to=2, border_from=3)
out$p
```

Then activity does not turn off down to 0 but settles at an intermediate value.

```{r}
loop_args_dict = longon_args_dict
loop_args_dict$W = loop_W

loop_net_dat = networkModel(loop_W, loop_args_dict)

data.frame(loop_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop short task stimulation

Looks weird but keeping it for later tests.

```{r}
loop_short_args_dict = loop_args_dict
loop_short_args_dict$tasktiming = task$stim
loop_short_args_dict$Tmax = max(task$time)
loop_short_args_dict$I = make_stimtimes(loop_short_args_dict$stim_node, loop_short_args_dict)$stimtimes

loop_short_net_dat = networkModel(loop_W, loop_short_args_dict)

data.frame(loop_short_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

# Task parameter recovery

## GLM framework

Ok so now that you have a sense of how things work in the network try the regression framework in the initial simple network to see if you can detect which node is stimulated versus which is not.

Based on the algebra [here](https://github.com/zenkavi/NetworkGLM/blob/master/simulations/NB0_Inverting_Runge_Kutte.ipynb).

$$x_{i}(t+dt) = (1-\frac{dt}{\tau_i}+\frac{dt^2}{2\tau_i^2}){x_{i}(t)} + \frac{dt}{2\tau_i}\Bigg[(1 - \frac{dt}{\tau_i})\Bigg(gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg) + gN_i(t+dt) + s\phi\Bigg((1-\frac{dt}{\tau_i})x_{i}(t) + \frac{dt}{\tau_i}\big[gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\big]\Bigg) + {I}_{i}(t+dt)\Bigg]$$

### Non-hub stimulation

```{r}
net_dat_debug = networkModel(W, cur_args_dict, debug=T)
```

"Uncorrected" GLM overestimates task parameter. Task betas for each node:

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
round(run_ucr_glm(net_dat_debug, task_reg = cur_args_dict$I[1,])$ucr_task_betas, 4)
```

"Extended" GLM recovers correct task parameter.

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$ext_task_betas, 4)
```

### Hub stimulation

```{r}
hub_net_dat_debug = networkModel(W, hub_args_dict, debug=T)
```

Extended GLM correctly recovers task parameters when the hub node is stimulated.

```{r}
print(paste0("Stim node is: ", hub_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(hub_net_dat_debug, task_reg = hub_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(hub_net_dat_debug, cur_args_dict, task_reg = hub_args_dict$I[2,])$ext_task_betas, 4)
```

### Longer stimulation

```{r}
longon_net_dat_debug = networkModel(W, longon_args_dict, debug=T)
```

**Extended GLM overcorrects non-stimulated nodes.**

```{r}
print(paste0("Stim node is: ", longon_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(longon_net_dat_debug, task_reg = longon_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_task_betas, 4)
```

What's going on with the overcorrected nodes? Why does the model think the estimate of x_t_dt is better when you multiply I_t_dt with -0.0001 instead of 0? Because when estimating the model for non-stimulated nodes we use the tasktiming as the task regressor even though the true value for this is 0. We make this choice because apriori we can't know or measure which nodes would be stim nodes. This choice affects three regressors: `I_t`, `I_t_dt` and `s_phi_ave`. The model tries to balance the false increase in all three of these regressors. The difference between the shapes of `I_t` and `I_t_dt` used in the network activity creation and the corrected regression are trivial (for non-stim nodes it should be 0 but the task regressor fed into the regression model is told that the task is on) and therefore not depicted below. The effect on `s_phi_ave` is harder to visualize so it is shown below. For a non-trivial number of time points the regression `s_phi_ave` is larger than the `s_phi_ave` used in the network activity creation. To balance this overestimation the model assigns negative coefficients to the other two task regressors which appears like an "overcorrection"

```{r  fig.asp=.3, fig.height=10}
tmp_df = run_ext_glm(longon_net_dat_debug, longon_args_dict, task_reg = longon_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, longon_net_dat_debug, longon_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1)) +
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

Why don't we observe the same problem when the stimulation is short? It might be because the short task doesn't lead to a systematic/large enough deviation between the network `s_phi_ave` and regression `s_phi_ave`

```{r fig.asp=.3, fig.height=10}
tmp_df = run_ext_glm(hub_net_dat_debug, hub_args_dict, task_reg = hub_args_dict$I[2,])$ext_mods[[1]]
resids_df = check_net_resids(1, hub_net_dat_debug, hub_args_dict, verbose= TRUE)

p1 = data.frame(network = resids_df$s_phi_ave, regression = tmp_df$s_phi_ave) %>%
  mutate(time = 1: n()) %>%
  gather(key, value, -time) %>%
  ggplot(aes(time, value, color=key)) +
  geom_point()+
  theme(legend.title = element_blank())+
  ylab("s_phi_ave")

p2 = data.frame(resids = resids_df$s_phi_ave, tmp = tmp_df$s_phi_ave) %>%
  ggplot(aes(resids, tmp)) +
  geom_point()+
  geom_abline(aes(intercept=0, slope=1))+
  ylab("Regression model") +
  xlab("Network model")

grid.arrange(p1, p2, ncol=2)
```

### Loop stimulation

```{r}
loop_net_dat_debug = networkModel(loop_W, loop_args_dict, debug=T)
```

Extended GLM overcorrects non-stimulated nodes. But this is due to the longer task and not due to the loop. See below for the same loop adjacency matrix with a shorter task. The corrected estimates are not over-corrected for non-stimulated nodes.

```{r}
print(paste0("Stim node is: ", loop_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(loop_net_dat_debug, task_reg = loop_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(loop_net_dat_debug, loop_args_dict, task_reg = loop_args_dict$I[2,])$ext_task_betas, 4)
```

### Loop short stimulation

```{r}
loop_short_net_dat_debug = networkModel(loop_W, loop_short_args_dict, debug=T)
```

```{r}
print(paste0("Stim node is: ", loop_short_args_dict$stim_node))
print("Uncorrected estimates:")
round(run_ucr_glm(loop_short_net_dat_debug, task_reg = loop_short_args_dict$I[2,])$ucr_task_betas, 4)
print("Corrected estimates:")
round(run_ext_glm(loop_short_net_dat_debug, loop_short_args_dict, task_reg = loop_short_args_dict$I[2,])$ext_task_betas, 4)
```

# Potential applicability issues

## Observable vs true adjacency matrix

What does the adjacency matrix look like if you estimate it from timeseries?

**Note**: I haven't even considered inhibitory connections yet.

True adjacency matrix

```{r}
true_am = plot_adj_mat(cur_args_dict$W)
true_am_plot = true_am$p+
  scale_fill_gradient(limits=c(-1,1)) +
  ggtitle("True")

mylegend<-g_legend(true_am_plot)

```

Correlation matrix from observed time series

```{r}
methods = c("pearson", "spearman", "kendall")
ps = list()
for(i in 1:length(methods)){
  ps[[i]] = true_am_plot +
    theme(legend.position = "none")+
    ggtitle("True")
  ps[[i+length(methods)]] = plot_adj_mat(cor(t(net_dat), method=methods[i]))$p +
    scale_fill_gradient(limits=c(-1,1))+
    ggtitle(methods[i])+
    theme(legend.position = "none")
  ps[[i+2*length(methods)]] = plot_adj_mat(ppcor::pcor(t(net_dat), method=methods[i])$estimate)$p +
    scale_fill_gradient(limits=c(-1,1)) +
    ggtitle(methods[i])+
    theme(legend.position = "none")
}
```

All three plots in first row are identical and third row is partial correlations.

```{r out.height='100%'}
grid.arrange(arrangeGrob(grobs = ps, nrow=3), mylegend, ncol=2, widths = c(10,1))
```

## Effect on ext_glm estimates

How are the estimates of ext_glm affected if you use the estimated adjacency matrix?

```{r}
args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = cur_args_dict
  args_dicts[[i]]$W = cor(t(net_dat), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = cur_args_dict
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(net_dat), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(net_dat, args_dicts[[i]], task_reg = cur_args_dict$I[1,])$ext_task_betas, 4))
}

```

Does using different connectivity matrices affect other scenarios?

Hub stimulation

```{r}
sub1 = hub_args_dict
sub2 = hub_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Longer task with hub stimulation

```{r}
sub1 = longon_args_dict
sub2 = longon_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Looped network with longer task and hub stimulation

```{r}
sub1 = loop_args_dict
sub2 = loop_net_dat

args_dicts = list()
for(i in 1:length(methods)){
  args_dicts[[i]] = sub1
  args_dicts[[i]]$W = cor(t(sub2), method=methods[i])
  args_dicts[[i]]$W_method = paste0(methods[i], "-full")
  args_dicts[[i+length(methods)]] = sub1
  args_dicts[[i+length(methods)]]$W = ppcor::pcor(t(sub2), method=methods[i])$estimate
  args_dicts[[i+length(methods)]]$W_method = paste0(methods[i], "-partial")

}

for(i in 1:length(args_dicts)){
  print(paste0("Stim node is: ", args_dicts[[i]]$stim_node))
  print(paste0("Corrected estimates using: ", args_dicts[[i]]$W_method))
  print(round(run_ext_glm(sub2, args_dicts[[i]], task_reg = sub1$I[2,])$ext_task_betas, 4))
}
```

Good news? Using different adjacency matrices doesn't seem to affect the results too much. How come?  

What regressors are affected by the change in the connectivity matrix? Given the model  

`mod = lm(x_t_dt ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + g_N_t_dt + s_phi_ave + I_t_dt)`  

affected regressors would be: `g_N_t`, `g_N_t_dt`, `s_phi_ave`  

So can you get significant improvement in estimating task involvement of each node just by accounting for self activity and not accounting for network activity? Yes.

```{r}
print(paste0("Stim node is: ", cur_args_dict$stim_node))
print("Uncorrected estimates")
round(run_ucr_glm(net_dat_debug, task_reg = cur_args_dict$I[1,])$ucr_task_betas, 4)
print("Corrected estimates with network activity")
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$ext_task_betas, 4)
print("Corrected estimates without network activity")
round(run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,], inc_net_act = FALSE)$ext_task_betas, 4)
```

Does the addition of the network activity regressors improve the model significantly? It does. But the parameter estimates for task involvement do not change drastically compared to not including them.

```{r}
out = run_ext_glm(net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])

for (i in 1:length(out$ext_mods)){
  cur_df = out$ext_mods[[i]]
  m1 = lm(x_t_dt ~ -1 + x_t + s_phi_x_t + I_t + I_t_dt, data = cur_df)
  m2 = lm(x_t_dt ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + g_N_t_dt + s_phi_ave + I_t_dt, data = cur_df)
  print(anova(m1, m2))
}
```

## Effect on larger network

Is the effect of accounting for the network activity in correcting the task involvement small bc the network is small? Would it be more substantial for a larger/more complicated network?

```{r}
big_W_args_dict = default_args_dict
big_G = generateStructuralNetwork(big_W_args_dict)
big_W_args_dict$W = generateSynapticNetwork(big_G$W, showplot=big_W_args_dict$showplot)$G
big_W_args_dict$stim_nodes = c(1:10)
big_W_args_dict$I = make_stimtimes(big_W_args_dict$stim_nodes, big_W_args_dict)$stimtimes
big_W_net_dat = networkModel(big_W_args_dict$W, big_W_args_dict)
big_W_net_dat_debug = networkModel(big_W_args_dict$W, big_W_args_dict, debug=T)
```

```{r}
plot_adj_mat(big_W_args_dict$W)$p
```

```{r}
data.frame(stim = make_stimtimes(big_W_args_dict$stim_nodes, big_W_args_dict)$tasktiming) %>%
  mutate(timepoint = 1:n())%>%
  ggplot(aes(timepoint, stim)) +
  geom_line()
```

```{r}
print(paste0("Stim node is: ", big_W_args_dict$stim_nodes))
print("Uncorrected estimates")
round(run_ucr_glm(big_W_net_dat_debug, task_reg = big_W_args_dict$I[1,])$ucr_task_betas, 4)
print("Corrected estimates with network activity")
round(run_ext_glm(big_W_net_dat_debug, big_W_args_dict, task_reg = big_W_args_dict$I[1,])$ext_task_betas, 4)
print("Corrected estimates without network activity")
round(run_ext_glm(big_W_net_dat_debug, big_W_args_dict, task_reg = big_W_args_dict$I[1,], inc_net_act = FALSE)$ext_task_betas, 4)
```
