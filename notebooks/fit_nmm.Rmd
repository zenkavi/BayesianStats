---
title: "Neural mass model simulation and external stimulation parameter recovery with Stan"
---

```{r}
library(tidygraph)
library(ggraph)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
source(paste0(helpers_path,'run_ucr_glm.R'))
source(paste0(helpers_path,'run_ext_glm.R'))
```

# Developing intuitions for activity propogation within a network

## Non-hub stimulation

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
edges %>%
  rbind(data.frame(from = c(1,2,3,3,1,3), to = c(1,2,3,1,3,2), weight = c(0,0,0,0,0,0))) %>%
  ggplot(aes(x=from, y=to, fill=weight))+
  geom_tile()
```

Suppose we have a task that looks like:

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))
task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

The task stimulates *only* node 1 directly and propagates within the network based on the following Eq:

$$\frac{dx_i}{dt}\tau_i = -x_i(t) + s\phi\big(x_i(t)\big) + g\Bigg(\sum_{j\neq i}^{N} W_{ij}\phi\big(x_j(t)\big)\Bigg) + I_i(t)$$
Activity in each node at each time point would look like:

```{r}
cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$I = make_stimtimes(1, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Hub stimulation

The stim node in the previous stimulations was **not** the hub. What if you stimulate the hub node? (In this network this would be node 2)

```{r}
hub_args_dict = cur_args_dict

hub_args_dict$I = make_stimtimes(2, hub_args_dict)$stimtimes

hub_net_dat = networkModel(W, hub_args_dict)

data.frame(hub_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Longer stimulation

What if the task is on for longer?

```{r}
longon_task = data.frame(stim = c(c(0,0,0), rep(1, 10), rep(0,100)))
longon_task$time = rep(1:nrow(longon_task))
longon_task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

```{r}
longon_args_dict = cur_args_dict
longon_args_dict$tasktiming = longon_task$stim
longon_args_dict$Tmax = max(longon_task$time)
longon_args_dict$I = make_stimtimes(2, longon_args_dict)$stimtimes

longon_net_dat = networkModel(W, longon_args_dict)

data.frame(longon_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

## Loop stimulation

What if you have complete loop?

```{r}
loop_W = W
loop_W[2, 3] = .2
loop_W
```

Such that the adjacency matrix changes to:

```{r}
data.frame(loop_W) %>%
  mutate(to = row.names(.),
         from = names(.),
         from = gsub("X","",from),
         to = as.numeric(to),
         from = as.numeric(to)) %>%
  gather(key, weight, -to, -from) %>%
  mutate(from = sort(from)) %>%
  select(-key) %>%
  mutate(border = ifelse(to == 2 & from == 3, T, NA)) %>%
  ggplot(aes(x=from, y=to, fill=weight, color=border))+
  geom_tile(size = 2) +
  scale_color_manual(guide=F, values = c('TRUE' = "red"))
```

Then activity does not turn off down to 0 but settles at an intermediate value.

```{r}
loop_args_dict = longon_args_dict
loop_args_dict$W = loop_W

loop_net_dat = networkModel(loop_W, loop_args_dict)

data.frame(loop_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

# Task parameter recovery

## GLM framework

Ok so now that you have a sense of how things work in the network try the regression framework in the initial simple network to see if you can detect which node is stimulated versus which is not.

Based on the algebra [here](https://github.com/zenkavi/NetworkGLM/blob/master/simulations/NB0_Inverting_Runge_Kutte.ipynb).

$$x_{i}(t+dt) = (1-\frac{dt}{\tau_i}+\frac{{dt}^2}{2{\tau_i}^2}){x_{i}(t)} + \frac{dt}{2\tau_i}\Bigg[(1 - \frac{dt}{\tau_i})\Bigg(gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg) + gN_i(t+dt) + s\phi\Bigg((1-\frac{dt}{\tau_i})x_{i}(t) + \frac{dt}{\tau_i}\big[gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\big]\Bigg) + {I}_{i}(t+dt)\Bigg]$$

```{r eval=FALSE}
# Confirm that the residuals are 0 for stimulated node. Stim node for this data is node 1. This should confirm the resulting equation from the algebra.

node = 1
x_t_dt = net_dat_debug$Enodes[node,-1]
dt = cur_args_dict$dt
tau = cur_args_dict$tau
const1 = 1 - (dt/tau) +  ((dt^2)/(2*tau^2))
x_t = net_dat_debug$Enodes[node, -ncol(net_dat_debug$Enodes)]
const2 = dt/(2*tau)
const2_1 = 1 - (dt/tau)
g_N_t = net_dat_debug$int_out$net_act1[node,]
s = cur_args_dict$s
s_phi_x_t = s * phi(x_t)
I_t = net_dat_debug$int_out$spont_act1[node,]
g_N_t_dt = net_dat_debug$int_out$net_act2[node,]
s_phi_ave = s * phi((const_2_1*x_t)+((dt/tau)*(g_N_t+s_phi_x_t+I_t)))
I_t_dt = net_dat_debug$int_out$spont_act2[node,]

rhs = (const1 * x_t) + const2 * (const2_1 * (g_N_t + s_phi_x_t + I_t) + g_N_t_dt + s_phi_ave + I_t_dt)
unique(round(x_t_dt - rhs, 10))
```

Unlike the regression framework in the python version first try it using the outputs from debug mode

```{r}
net_dat_debug = networkModel(W, cur_args_dict, debug=T)
```

"Uncorrected" GLM overestimates task parameter. Task betas for each node

```{r}
round(run_ucr_glm_node(1, net_dat_debug)$coef, 4)
```

"Corrected" GLM recovers task parameter.

```{r}
run_ext_glm_node(1,net_dat_debug, cur_args_dict)$coef
```

How about the other nodes? Need to use the stim time for stimulated node bc otherwise the regressor is only 0.

```{r}
round(run_ucr_glm_node(2, net_dat_debug, task_reg = cur_args_dict$I[1,])$coef, 4)
round(run_ext_glm_node(2, net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$coef, 4)
```

```{r}
round(run_ucr_glm_node(3, net_dat_debug, task_reg = cur_args_dict$I[1,])$coef, 4)
round(run_ext_glm_node(3, net_dat_debug, cur_args_dict, task_reg = cur_args_dict$I[1,])$coef, 4)
```

Test function for all nodes with and without task_reg specified.

```{r}

```

Other datasets to try the regression framework on later

```{r}
hub_net_dat_debug = networkModel(W, hub_args_dict, debug=T)
longon_net_dat_debug = networkModel(W, longon_args_dict, debug=T)
loop_net_dat_debug = networkModel(loop_W, loop_args_dict, debug=T)
```

Conclusion on regression framework attempt:

## Bayesian parameter recovery with Stan

Move on to estimating task parameters using Stan. [state-space models]

```{r}

```

------

Question re estimation methods: Are the posteriors unimodal in VI vs MCMC? Are we losing important information when making assumptions like the Laplace approximation?
