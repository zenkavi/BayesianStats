---
title: "Neural mass model simulation and parameter recovery with Stan"
---

```{r}
library(tidygraph)
library(ggraph)
```

```{r}
helpers_path = '~/Dropbox/RangelLab/NetworkGLM/helpers/r_helpers/'
source(paste0(helpers_path,'networkModel.R'))
source(paste0(helpers_path,'make_stimtimes.R'))
source(paste0(helpers_path,'generateStructuralNetwork.R'))
source(paste0(helpers_path,'generateSynapticNetwork.R'))
```

Suppose we have a three node network that looks like:

```{r}
edges = data.frame(from = c(1, 2, 2), to = c(2, 1, 3), weight = c(.4, .2, .3))
nodes = data.frame(id = c(1,2,3), label = c("1", "2", "3"))
```

```{r}
min_net = tbl_graph(nodes=nodes, edges=edges, directed=T)
```

```{r}
ggraph(min_net, layout="circle")+
  geom_edge_parallel(aes(width=weight, label=weight), 
                 alpha=.8,
                 arrow = arrow(length = unit(4, 'mm')),
                 end_cap = circle(5, 'mm'),
                 start_cap = circle(5, 'mm'),
                 label_dodge=unit(-4.0,"mm"),
                 label_push=unit(4,"mm"),
                 position="identity",angle_calc="along",force_flip=T)+
  scale_edge_width(range=c(.2,2))+
  geom_node_point(size=7)+
  geom_node_label(aes(label=label), 
                 repel=T)+ 
  theme_graph()+
  theme(legend.position = "none",
        plot.margin = margin(0, 1, .5, 1, "cm"))
```

Adjacency matrix for the above minimal network looks like:

```{r}
W<-matrix(0, 3, 3)
W[as.matrix(edges[,2:1])] <- edges$weight
W
```

```{r}
edges %>%
  rbind(data.frame(from = c(1,2,3,3,1,3), to = c(1,2,3,1,3,2), weight = c(0,0,0,0,0,0))) %>%
  ggplot(aes(x=from, y=to, fill=weight))+
  geom_tile()
```

Suppose we have a task that looks like:

```{r}
task = data.frame(stim = c(c(0,0,0,1),rep(0,97)))
task$time = rep(1:nrow(task))
task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

The task stimulates *only* node 1 directly and propagates within the network based on the following Eq:

$$\frac{dx_i}{dt}\tau_i = -x_i(t) + s\phi\big(x_i(t)\big) + g\Bigg(\sum_{j\neq i}^{N} W_{ij}\phi\big(x_j(t)\big)\Bigg) + I_i(t)$$
Activity in each node at each time point would look like:

```{r}
cur_args_dict = list('dt'=.5,  
                 'g'=1, 
                 'noise'= NULL,
                 'noise_loc'= 0, 
                 'noise_scale'= 0,
                 's'=.7,
                 'stim_mag'=.5,
                 'taskdata'=NULL,
                 'tasktiming'=task$stim,
                 'tau'=1, 
                 'Tmax'=max(task$time),
                 'W'= W)

cur_args_dict$I = make_stimtimes(1, cur_args_dict)$stimtimes

net_dat = networkModel(W, cur_args_dict)

data.frame(net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

The stim node in the previous stimulations was **not** the hub. What if you stimulate the hub node? (In this network this would be node 2)

```{r}
hub_args_dict = cur_args_dict

hub_args_dict$I = make_stimtimes(2, hub_args_dict)$stimtimes

hub_net_dat = networkModel(W, hub_args_dict)

data.frame(hub_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```
What if the task is on for longer?

```{r}
longon_task = data.frame(stim = c(c(0,0,0), rep(1, 10), rep(0,100)))
longon_task$time = rep(1:nrow(longon_task))
longon_task %>%
  ggplot(aes(x=time, y=stim))+
  geom_line()+
  xlab("")+
  ylab("")+
  scale_y_continuous(breaks = c(0,1))
```

```{r}
longon_args_dict = cur_args_dict
longon_args_dict$tasktiming = longon_task$stim
longon_args_dict$Tmax = max(longon_task$time)
longon_args_dict$I = make_stimtimes(2, longon_args_dict)$stimtimes

longon_net_dat = networkModel(W, longon_args_dict)

data.frame(longon_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

What if you have complete loop?

```{r}
loop_W = W
loop_W[2, 3] = .2
loop_W
```

Such that the adjacency matrix changes to:

```{r}
data.frame(loop_W) %>%
  mutate(to = row.names(.),
         from = names(.),
         from = gsub("X","",from),
         to = as.numeric(to),
         from = as.numeric(to)) %>%
  gather(key, weight, -to, -from) %>%
  mutate(from = sort(from)) %>%
  select(-key) %>%
  mutate(border = ifelse(to == 2 & from == 3, T, NA)) %>%
  ggplot(aes(x=from, y=to, fill=weight, color=border))+
  geom_tile(size = 2) +
  scale_color_manual(guide=F, values = c('TRUE' = "red"))
```

Then activity does not turn off down to 0 but settles at an intermediate value.

```{r}
loop_args_dict = longon_args_dict
loop_args_dict$W = loop_W

loop_net_dat = networkModel(loop_W, loop_args_dict)

data.frame(loop_net_dat) %>%
  mutate(node = c(1,2,3)) %>%
  gather(sampling, value, -node) %>%
  mutate(sampling = gsub("X", "", sampling),
         sampling = as.numeric(sampling),
         node = as.factor(node)) %>%
  ggplot(aes(x=sampling, y=value, color = node))+
  geom_line()
```

Ok so now that you have a sense of how things work in the network try the regression framework in the initial simple network to see if you can detect which node is stimulated versus which is not.

Based on the algebra [here](https://github.com/zenkavi/NetworkGLM/blob/master/simulations/NB0_Inverting_Runge_Kutte.ipynb).

$$x_{i}(t+dt) = (1-\frac{dt}{\tau_i}){x_{i}(t)} + \frac{dt}{2\tau_i}\Bigg[gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t) - \frac{dt}{\tau_i}\Bigg[-x_{i}(t) + gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg] + gN_i(t+dt) + s\phi\Bigg(x_{i}(t) + \frac{dt}{\tau_i}\Bigg[-x_{i}(t) + gN_i(t) + s\phi\big(x_{i}(t)\big) + {I}_{i}(t)\Bigg]\Bigg) + {I}_{i}(t+dt)\Bigg]$$

Unlike the regression framework in the python version first try it using the outputs from debug mode

```{r}
net_dat_debug = networkModel(W, cur_args_dict, debug=T)
# Other datasets to try the regression framework on later
# hub_net_dat = networkModel(W, hub_args_dict)
# longon_net_dat = networkModel(W, longon_args_dict)
# loop_net_dat = networkModel(loop_W, loop_args_dict)
```

Confirm that the residuals are 0 for stimulated node. Stim node for this data is node 1. This should confirm the resulting equation from the algebra.

```{r}
node = 1
x_t_dt = net_dat_debug$Enodes[node,-1]
dt = cur_args_dict$dt
tau = cur_args_dict$tau
const1 = 1 - (dt/tau)
x_t = net_dat_debug$Enodes[node, -ncol(net_dat_debug$Enodes)]
const2 = dt/(2*tau)
g_N_t = net_dat_debug$int_out$net_act1[node,]
s = cur_args_dict$s
s_phi_x_t = s * phi(x_t)
I_t = net_dat_debug$int_out$spont_act1[node,]
dt_tau = (-1)*(dt/tau)*(-x_t+g_N_t+s_phi_x_t+I_t)
g_N_t_dt = net_dat_debug$int_out$net_act2[node,]
s_phi_ave = s * phi(x_t - dt_tau)
I_t_dt = net_dat_debug$int_out$spont_act2[node,]

rhs = (const1 * x_t) + const2 * (g_N_t + s_phi_x_t + I_t + dt_tau + g_N_t_dt + s_phi_ave + I_t_dt)
data.frame(resid = round(x_t_dt - rhs), 10) %>%
  mutate(time = 1: n()) %>%
  ggplot(aes(time, resid))+
  geom_point()
```

Can I recover the constants if I run a GLM?
Why is the coef of dt_tau 0?
Why do the coefs change so drastically when we add some noise

```{r}
mod = lm(x_t_dt + rnorm(length(x_t_dt), 0, .01) ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + dt_tau + g_N_t_dt + s_phi_ave + I_t_dt)
mod = lm(x_t_dt ~ -1 + x_t + g_N_t + s_phi_x_t + I_t + dt_tau + g_N_t_dt + s_phi_ave + I_t_dt)
summary(mod)
coef(mod)
```

```{r}
data.frame(dt_tau) %>%
  mutate(time = 1:n()) %>%
  ggplot(aes(time, dt_tau))+
  geom_point()
```

```{r}
summary(lm(x_t ~ I_t))
```

Conclusion on regression framework attempt:

Move on to estimating task parameters using Stan. [state-space models]

```{r}

```

------

Question re estimation methods: Are the posteriors unimodal in VI vs MCMC? Are we losing important information when making assumptions like the Laplace approximation?
