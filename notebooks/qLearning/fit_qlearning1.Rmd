---
title: "RL simulation and parameter recovery with Stan I: Single subject parameter recovery"
---

Translating Deanizeau et al. (2014) Q-learning example

```{r}
source('/Users/zeynepenkavi/Dropbox/RangelLab/IntroToStan//helpers/demo_QlearningSimulation.R')
```

## Example 1: Nice true parameter values

### Data

Generate simulated data using a model with a single learning `alpha` and inverse temperature parameter `beta`. For this example using `alpha = .65` and `beta = 2.5`.

```{r}
sim_data = demo_QlearningSimulation()
```

Figure 1: Simulated Q-learning behavior (from [demo_Qlearning.m](https://github.com/MBB-team/VBA-toolbox/blob/master/demos/3_behavioural/demo_Qlearning.m)) for 2 armed bandit task with .75 reward probability and changing contingencies every 25 trials.

```{r}
df = data.frame(choices = c(sim_data$y), 
                tendency = c(sim_data$y - sim_data$e))
```

```{r}
df %>%
  mutate(trial_n = 1:n()) %>%
  ggplot(aes(x=trial_n))+
  geom_jitter(aes(y=choices, color="choices"), width=.01, height=.01)+
  geom_line(aes(y=tendency, color="p(y=1|theta,phi,m): behavioural tendency"))+
  scale_colour_manual(name="",values=c("choices" = "black", "p(y=1|theta,phi,m): behavioural tendency" = "red"))+
  theme(legend.position = "bottom")
```

### Parameter estimation

Parameters for this dataset:  

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Now that you have the data invert it using Stan to get parameters.

```{r}
TN = length(sim_data$choice)
choice = sim_data$choices #adding 1 bc categorical_logit has support over [1,2]
outcome = sim_data$feedback
m_data = list(T = TN, choice = choice, outcome = outcome)
```

Priors: `beta(1,1)` for `alpha` and `gamma(1,2)` for `beta`

```{r}
m = stan_model('stanModels/QLearning_singleSubject.stan')
```

```{r}
fit_nuts = sampling(m, iter=1000, chains=4, data=m_data)
```

```{r}
fit_vb = vb(m, data=m_data)
```

#### Comparison of posteriors

Neither inversion method recovers parameters perfectly (but much better than my previous simulation attempts using MLE).

```{r}
data.frame(extract(fit_nuts, c("alpha", "beta"))) %>%
  mutate(alg = "NUTS") %>%
  gather(key, value, -alg) %>%
  group_by(key) %>%
  mutate(med_est = median(value),
         true_val = ifelse(key == "alpha", true_alpha, true_beta)) %>%
  
  rbind(data.frame(extract(fit_vb, c("alpha", "beta"))) %>%
          mutate(alg = "ADVI") %>%
          gather(key, value, -alg) %>%
          group_by(key) %>%
          mutate(med_est = median(value),
                 true_val = ifelse(key == "alpha", true_alpha, true_beta))) %>%
  gather(est_type, estimate, -alg, -key, -value) %>%
  
  ggplot(aes(x = value, fill=alg))+
  geom_histogram(position="identity", alpha = .5)+
  facet_grid(alg~key, scales='free')+
  geom_vline(aes(xintercept=estimate, linetype = est_type)) +
  scale_linetype_manual(name="", values=c("med_est" = "solid", "true_val" = "dashed"),
                        labels = c("med_est" = "median estimate", "true_val" = "true value"))+
  scale_fill_manual(name="", values=c("NUTS" = "purple", "ADVI" = "dark green"))+
  theme(legend.position="bottom")+
  xlab("")+
  ylab("")
```


```{r}
summary(fit_nuts, pars = c('alpha', 'beta', 'log_lik'))$summary
```

According to [Yao et al. (2018)](http://www.stat.columbia.edu/~gelman/research/published/Evaluating_Variational_Inference.pdf) `khat` "provide[s] the desired diagnostic measurement between the true posterior 
and the VI approximation"

```{r}
summary(fit_vb, pars = c('alpha', 'beta', 'log_lik'))$summary
```

#### Pair plots colored by log likelihood

Pair plots show that due to the independence assumption built into ADVI the posteriors of parameter estimates are uncorrelated. The NUTS samples show that the independence assumption is violated and that the parameters are negatively correlated. This dependency leads to unidentifiability (i.e. there are multiple combinations of the parameters that are equally likely to generate the observed data).  

```{r}
data.frame(extract(fit_nuts, c('alpha', 'beta', 'log_lik'))) %>%
  mutate(alg = "NUTS",
         est_alpha = median(alpha),
         est_beta = median(beta)) %>%
  rbind(data.frame(extract(fit_vb, c('alpha', 'beta', 'log_lik'))) %>%
  mutate(alg = "ADVI",
         est_alpha = median(alpha),
         est_beta = median(beta)))%>%
  mutate(true_alpha = true_alpha, true_beta = true_beta)%>%
  ggplot()+
  geom_point(aes(x=alpha, y=beta, color=log_lik)) +
  scale_colour_gradient()+
  geom_point(aes(x=true_alpha, y = true_beta), color="red", shape = 13, size=6)+
  geom_point(aes(x=est_alpha, y = est_beta), color =  "black", shape=4, size=6)+
  facet_grid(~alg)+
  theme(legend.position = "bottom")+
  ylab("Inverse temperature")+
  xlab("Learning rate")
```

## Example 2: High learning rate lower beta

### Data

```{r}
sim_data = demo_QlearningSimulation(alpha = .9, beta = .7)
```

### Parameter estimation

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Use Stan to get parameters that generated simulated data.

```{r}
TN = length(sim_data$choice)
choice = sim_data$choices #adding 1 bc categorical_logit has support over [1,2]
outcome = sim_data$feedback
m_data = list(T = TN, choice = choice, outcome = outcome)
```

```{r}
fit_nuts = sampling(m, iter=1000, chains=4, data=m_data)
```

```{r}
data.frame(extract(fit_nuts, c("alpha", "beta"))) %>%
  mutate(alg = "NUTS") %>%
  gather(key, value, -alg) %>%
  group_by(key) %>%
  mutate(med_est = median(value),
         true_val = ifelse(key == "alpha", true_alpha, true_beta)) %>%

  gather(est_type, estimate, -alg, -key, -value) %>%
  
  ggplot(aes(x = value, fill=alg))+
  geom_histogram(position="identity", alpha = .5)+
  facet_grid(alg~key, scales='free')+
  geom_vline(aes(xintercept=estimate, linetype = est_type)) +
  scale_linetype_manual(name="", values=c("med_est" = "solid", "true_val" = "dashed"),
                        labels = c("med_est" = "median estimate", "true_val" = "true value"))+
  scale_fill_manual(name="", values=c("NUTS" = "purple"))+
  theme(legend.position="bottom")+
  xlab("")+
  ylab("")
```

```{r}
data.frame(extract(fit_nuts, c('alpha', 'beta', 'log_lik'))) %>%
  mutate(alg = "NUTS",
         est_alpha = median(alpha),
         est_beta = median(beta)) %>%
  mutate(true_alpha = true_alpha, true_beta = true_beta)%>%
  ggplot()+
  geom_point(aes(x=alpha, y=beta, color=log_lik)) +
  geom_point(aes(x=true_alpha, y = true_beta), color="red", shape = 13, size=6)+
  geom_point(aes(x=est_alpha, y = est_beta), color =  "black", shape=4, size=6)+
  theme(legend.position = "bottom")+
  ylab("Inverse temperature")+
  xlab("Learning rate")
```

## Example 3: Low learning rate high beta

### Data

```{r}
sim_data = demo_QlearningSimulation(alpha = .15, beta = 3)
```

### Parameter estimation

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Now that you have the data invert it using Stan to get parameters.

```{r}
TN = length(sim_data$choice)
choice = sim_data$choices #adding 1 bc categorical_logit has support over [1,2]
outcome = sim_data$feedback
m_data = list(T = TN, choice = choice, outcome = outcome)
```

```{r}
fit_nuts = sampling(m, iter=1000, chains=4, data=m_data)
```

```{r}
data.frame(extract(fit_nuts, c("alpha", "beta"))) %>%
  mutate(alg = "NUTS") %>%
  gather(key, value, -alg) %>%
  group_by(key) %>%
  mutate(med_est = median(value),
         true_val = ifelse(key == "alpha", true_alpha, true_beta)) %>%

  gather(est_type, estimate, -alg, -key, -value) %>%
  
  ggplot(aes(x = value, fill=alg))+
  geom_histogram(position="identity", alpha = .5)+
  facet_grid(alg~key, scales='free')+
  geom_vline(aes(xintercept=estimate, linetype = est_type)) +
  scale_linetype_manual(name="", values=c("med_est" = "solid", "true_val" = "dashed"),
                        labels = c("med_est" = "median estimate", "true_val" = "true value"))+
  scale_fill_manual(name="", values=c("NUTS" = "purple"))+
  theme(legend.position="bottom")+
  xlab("")+
  ylab("")
```


```{r}
data.frame(extract(fit_nuts, c('alpha', 'beta', 'log_lik'))) %>%
  mutate(alg = "NUTS",
         est_alpha = median(alpha),
         est_beta = median(beta)) %>%
  mutate(true_alpha = true_alpha, true_beta = true_beta)%>%
  ggplot()+
  geom_point(aes(x=alpha, y=beta, color=log_lik)) +
  geom_point(aes(x=true_alpha, y = true_beta), color="red", shape = 13, size=6)+
  geom_point(aes(x=est_alpha, y = est_beta), color =  "black", shape=4, size=6)+
  theme(legend.position = "bottom")+
  ylab("Inverse temperature")+
  xlab("Learning rate")
```
