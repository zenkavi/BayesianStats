---
title: "RL simulation and parameter recovery with Stan III: Model comparison"
---

**Even if we can't do a good job with estimating the exact parameters for subjects can we distinguish between (cognitive) models and reliably identify the correct data generating process?**

```{r}
source('/Users/zeynepenkavi/Dropbox/RangelLab/BayesianStats/helpers/demo_QlearningSimulation.R')
library(loo)
library(caret)
```

```{r}
sim_data = demo_QlearningSimulation(alpha = .65, beta = 2.5)
```

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Using `caret` to make folds for cross validation.

```{r}
testIndices = createFolds(sim_data$choices, k = 5, list=TRUE, returnTrain = FALSE)
```

```{r}
## Adapted from Lambert textbook p. 397
k_fold = function(aModel, testIndices, simData){
  numFolds = length(testIndices)
  
  #expected log pointwise predictive density
  elpd = 0
  
  for(i in 1:numFolds){
    cur_ind = testIndices[[i]]
  
    T_train = length(sim_data$choices) - length(cur_ind)
    T_test = length(cur_ind)
  
    choice_train = simData$choices[-cur_ind] #adding 1 bc categorical_logit has support over [1,2]
    choice_test = simData$choices[cur_ind]
    
    outcome_train = simData$feedback[-cur_ind] 
    outcome_test = simData$feedback[cur_ind] 
    
    m_data = list(T_train = T_train, T_test = T_test, 
                  choice_train = choice_train, choice_test = choice_test, 
                  outcome_train = outcome_train, outcome_test = outcome_test)
    
    fit = sampling(aModel, iter=1000, chains=4, data=m_data, show_messages=FALSE, verbose=FALSE)
    
    fold_llpd = extract_log_lik(fit, 'log_lik') #log likelihood for each sample (1000) and observations in this fold
    
    #Based on https://discourse.mc-stan.org/t/calculating-elpd-values-for-a-stan-model-k-fold-cross-validation/3570/7
    #fold_llpd = exp(fold_llpd) #exponentiate each element
    fold_llpd_means = colMeans(fold_llpd) # mean likelihood across samples for each y_test in this fold
    #fold_llpd_means = log(fold_llpd_means)
    fold_elpd_increment = sum(fold_llpd_means)
    elpd = c(elpd, fold_elpd_increment)
    
    #For a standard error you need standard error of a sum i.e. sd*sqrt(n)
  }
  
  return(list(elpd_sum = sum(elpd), elpd_se = sd(elpd)*sqrt(length(elpd))))
}
```

Use `stan` to get parameters that generated simulated data.

```{r}
m_true = stan_model('../stanModels/qLearning/QLearning_cv_trueModel.stan')
```

```{r}
m_twoAlphas = stan_model('../stanModels/qLearning/QLearning_cv_twoAlphas.stan')
m_diffAlphas = stan_model('../stanModels/qLearning/QLearning_cv_diffAlphas.stan')
m_strawMan = stan_model('../stanModels/qLearning/QLearning_cv_strawMan.stan')
```

```{r}
ELPD_true = k_fold(m_true, testIndices, sim_data)
```

```{r}
ELPD_twoAlphas = k_fold(m_twoAlphas, testIndices, sim_data)
```

```{r}
ELPD_diffAlphas = k_fold(m_diffAlphas, testIndices, sim_data)
```

```{r}
ELPD_strawMan = k_fold(m_strawMan, testIndices, sim_data)
```

```{r}
data.frame(ELPD_strawMan) %>%
  mutate(model = "straw man") %>%
  rbind(data.frame(ELPD_diffAlphas) %>%
  mutate(model = "diff Alphas")) %>%
  rbind(data.frame(ELPD_twoAlphas) %>%
  mutate(model = "two Alphas")) %>%
  rbind(data.frame(ELPD_true) %>%
  mutate(model = "true")) %>%
  mutate(model = factor(model, levels = c("true", "two Alphas", "diff Alphas", "straw man"), labels = c("true", "two Alphas", "diff Alphas", "random"))) %>%
  ggplot(aes(model))+
  geom_point(aes(y=elpd_sum), size=4)+
  geom_errorbar(aes(ymin = elpd_sum-elpd_se, ymax = elpd_sum+elpd_se, width=0))+
  xlab("")+
  ylab("ELPD")
```

Bridge sampling to calculate marginal likelihood and compute $p(m|y)$ for each model
