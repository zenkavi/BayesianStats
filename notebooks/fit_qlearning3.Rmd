---
title: "RL simulation and parameter recovery with Stan III: Model comparison"
---

**Even if we can't do a good job with estimating the exact parameters for subjects can we distinguish between (cognitive) models and reliably identify the correct data generating process?**

```{r}
source('/Users/zeynepenkavi/Dropbox/RangelLab/BayesianStats/helpers/demo_QlearningSimulation.R')
library(loo)
library(caret)
```

```{r}
sim_data = demo_QlearningSimulation(alpha = .65, beta = 2.5)
```

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Using `caret` to make folds for cross validation.

```{r}
testIndices = createFolds(sim_data$choices, k = 5, list=TRUE, returnTrain = FALSE)
```


```{r}
## Adapted from Lambert textbook p. 397
k_fold = function(aModel, testIndices, simData){
  numFolds = length(testIndices)
  
  #expected log pointwise predictive density
  lPointLogLikelihoodTotal = vector()
  
  for(i in 1:numFolds){
    cur_ind = testIndices[[i]]
  
    T_train = length(sim_data$choices) - length(cur_ind)
    T_test = length(cur_ind)
  
    choice_train = simData$choices[-cur_ind] #adding 1 bc categorical_logit has support over [1,2]
    choice_test = simData$choices[cur_ind]
    
    outcome_train = simData$feedback[-cur_ind] 
    outcome_test = simData$feedback[cur_ind] 
    
    m_data = list(T_train = T_train, T_test = T_test, 
                  choice_train = choice_train, choice_test = choice_test, 
                  outcome_train = outcome_train, outcome_test = outcome_test)
    
    fit = sampling(aModel, iter=1000, chains=4, data=m_data)
    
    logLikelihood1 = extract_log_lik(fit, 'log_lik') #log likelihood for each sample (1000) in this fold
    lPointLogLikelihood1 = colMeans(logLikelihood1) # mean log likelihood for this fold
    lPointLogLikelihoodTotal = c(lPointLogLikelihoodTotal, lPointLogLikelihood1)
  }
  #return(lPointLogLikelihoodTotal) #vector of mean log likelihoods of each fold
  return(list(lPointLogLikelihoodTotal=lPointLogLikelihoodTotal, last_fit=fit))
}
```

Use `stan` to get parameters that generated simulated data.

```{r}
m_true = stan_model('../stanModels/qLearning/QLearning_cv_trueModel.stan')
```

```{r}
m_twoAlphas = stan_model('../stanModels/qLearning/QLearning_cv_twoAlphas.stan')
m_diffAlphas = stan_model('../stanModels/qLearning/QLearning_cv_diffAlphas.stan')
m_strawMan = stan_model('../stanModels/qLearning/QLearning_cv_strawMan.stan')
```

See page 227 of Lambert textbook for definition of `elpd` (expected log predictive density) - "quantifies how close the estimated posterior predictive distribution, p(y_new|y) is to the true distribution f(y)." So larger is better (see page 228 for visual).  
`p(y_new | y)` = estimated posterior predictive distribution
`elpd` = single number resulting from summing the product of true density and the posterior predictive density for each value of y_new.
`elppd` = expected log pointwise predictive density `elpd` for `n` new data points

```{r}
ELPD_true = k_fold(m_true, testIndices, sim_data)
```

```{r}
ELPD_twoAlphas = k_fold(m_twoAlphas, testIndices, sim_data)
```

```{r}
ELPD_diffAlphas = k_fold(m_diffAlphas, testIndices, sim_data)
```

```{r}
ELPD_strawMan = k_fold(m_strawMan, testIndices, sim_data)
```


```{r}
sum(lELPD1)
```


Can it distinguish it significantly from the random choosing model? No.

```{r}
difference = sum(lELPD1) - sum(lELPD3)
sd = sqrt(149)*sd(lELPD1 - lELPD3)
1-pnorm(difference/sd)
```
