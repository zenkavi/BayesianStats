---
title: "RL simulation and parameter recovery with Stan III: Model comparison"
---

**Even if we can't do a good job with estimating the exact parameters for subjects can we distinguish between (cognitive) models and reliably identify the correct data generating process?**

```{r}
source('/Users/zeynepenkavi/Dropbox/RangelLab/BayesianStats/helpers/demo_QlearningSimulation.R')
library(loo)
library(caret)
```

```{r}
sim_data = demo_QlearningSimulation(alpha = .65, beta = 2.5)
```

True learning rate `alpha`

```{r}
true_alpha = VBA_sigmoid(sim_data$simulation$evolution, inverse=FALSE)
true_alpha
```

True inverse temperature `beta`

```{r}
true_beta = exp(sim_data$simulation$observation)
true_beta
```

Use `caret` to make folds for cross validation.

```{r}
testIndices = createFolds(sim_data$choices, k = 5, list=TRUE, returnTrain = FALSE)
```



```{r}
## Adapted from Lambert textbook p. 397
k_fold = function(aModel, testIndices, simData){
  numFolds = length(testIndices)
  
  #expected log pointwise predictive density
  lPointLogLikelihoodTotal = vector()
  
  for(i in 1:numFolds){
    cur_ind = testIndices[[i]]
  
    T_train = length(sim_data$choices) - length(cur_ind)
    T_test = length(cur_ind)
  
    choice_train = sim_data$choices[-cur_ind] #adding 1 bc categorical_logit has support over [1,2]
    choice_test = sim_data$choices[cur_ind]
    
    outcome_train = sim_data$feedback[-cur_ind] 
    outcome_test = sim_data$feedback[cur_ind] 
    
    m_data = list(T_train = T_train, T_test = T_test, 
                  choice_train = choice_train, choice_test = choice_test, 
                  outcome_train = outcome_train, outcome_test = outcome_test)
    
    fit = sampling(aModel, iter=1000, chains=4, data=m_data)
    
    logLikelihood1 = extract_log_lik(fit, 'log_lik') #log likelihood for each sample (1000) in this fold
    lPointLogLikelihood1 = colMeans(logLikelihood1) # mean log likelihood for this fold
    lPointLogLikelihoodTotal = c(lPointLogLikelihoodTotal, lPointLogLikelihood1)
  }
  #return(lPointLogLikelihoodTotal) #vector of mean log likelihoods of each fold
  return(list(lPointLogLikelihoodTotal=lPointLogLikelihoodTotal, last_fit=fit))
}
```

Use `stan` to get parameters that generated simulated data.

```{r}
m_true = stan_model('../stanModels/qLearning/QLearning_cv_trueModel.stan')
```

```{r}
m_false = stan_model('../stanModels/qLearning/QLearning_cv_falseModel.stan')
```


```{r}
out = k_fold(m_true, testIndices, sim_data)
```

```{r}
sum(extract_log_lik(out$last_fit, "log_lik"))
```

```{r}
str(extract_log_lik(out$last_fit, "log_lik"))
```

```{r}
out2 = k_fold(m_true, testIndices, sim_data)
```

```{r}
str(extract_log_lik(out2$last_fit, "log_lik"))
```

```{r}
sum(extract_log_lik(out2$last_fit, "log_lik"))
```

See page 227 of Lambert textbook for definition of `elpd` (expected log predictive density) - "quantifies how close the estimated posterior predictive distribution, p(y_new|y) is to the true distribution f(y)." So larger is better (see page 228 for visual).  
`p(y_new | y)` = estimated posterior predictive distribution
`elpd` = single number resulting from summing the product of true density and the posterior predictive density for each value of y_new.
`elppd` = expected log pointwise predictive density `elpd` for `n` new data points

```{r}
lELPD1 = k_fold(m_true, testIndices, sim_data)
```

```{r}
lELPD2 = k_fold(m_false, testIndices, sim_data)
```

```{r}
sum(lELPD1)
```

```{r}
sum(lELPD2)
```

Are they indistinguishable because the model estimates the same value for the two learning rates since there is no constraint on them? Yes.

```{r}
T_train = length(sim_data$choice)
choice = sim_data$choices #adding 1 bc categorical_logit has support over [1,2]
outcome = sim_data$feedback
m_data = list(T_train = T_train, choice_train = choice, outcome_train = outcome)
```

```{r}
m = stan_model('../stanModels/qLearning/QLearning_allData_falseModel.stan')
```

```{r}
fit_nuts = sampling(m, iter=1000, chains=4, data=m_data)
```

```{r}
summary(fit_nuts, pars = c('alpha_pos', 'alpha_neg', 'beta', 'log_lik'))$summary
```

Is model comparison more successful against a straw man model of random choosing?

```{r}
m_straw = stan_model('../stanModels/qLearning/QLearning_strawManModel.stan')
```

```{r}
lELPD3 = k_fold(m_straw, testIndices, sim_data)
```

```{r}
sum(lELPD1)
```

```{r}
sum(lELPD3)
```

Can it distinguish it significantly from the random choosing model? No.

```{r}
difference = sum(lELPD1) - sum(lELPD3)
sd = sqrt(149)*sd(lELPD1 - lELPD3)
1-pnorm(difference/sd)
```

What if you constrain the learning rates to be different from each other? (see model on p 403 for an example use of the `ordered` class)

```{r}
m_diffAlphas = stan_model('../stanModels/qLearning/QLearning_cv_diffAlphas.stan')
```

```{r}
lELPD4 = k_fold(m_diffAlphas, testIndices, sim_data)
```

```{r}
sum(lELPD4)
```